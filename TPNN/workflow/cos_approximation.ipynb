{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from TPNN.tools.perceptron import*\n",
    "import numpy as np\n",
    "import numpy.random as rand"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test net"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "layers=3\n",
      "[0]\n",
      " size=2\n",
      " act_function=<function ident at 0x061726A0>\n",
      " activations=[[0. 0.]]\n",
      " biases=[[0. 0.]]\n",
      " weights:\n",
      "     empty\n",
      "--------------------------\n",
      "[1]\n",
      " size=5\n",
      " act_function=<function sigmoid at 0x0A520778>\n",
      " activations=[[0. 0. 0. 0. 0.]]\n",
      " biases=[[1 1 1 1 1]]\n",
      " weights:\n",
      "     |1 1 1 1 1 |\n",
      "     |1 1 1 1 1 |\n",
      "\n",
      "--------------------------\n",
      "[2]\n",
      " size=1\n",
      " act_function=<function ident at 0x061726A0>\n",
      " activations=[[0.]]\n",
      " biases=[[0]]\n",
      " weights:\n",
      "     |1 |\n",
      "     |1 |\n",
      "     |1 |\n",
      "     |1 |\n",
      "     |1 |\n",
      "\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "net.insert_layer(0, 2, ident)\n",
    "net.insert_layer(1, 5, sigmoid)\n",
    "net.insert_layer(2, 1, ident)\n",
    "print(net.layers_count)\n",
    "net.init_weights(1, np.array([[1,1,1,1,1], [1,1,1,1,1]]))\n",
    "net.init_weights(2, np.array([[1,1,1,1,1]]).transpose())\n",
    "net.init_biases(2, np.array([[0]]))\n",
    "net.init_biases(1, np.array([[1,1,1,1,1]]))\n",
    "\n",
    "net.print_net_config()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[4.76287063]])"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.calc_output(np.array([1, 1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers=4\n",
      "[0]\n",
      " size=5\n",
      " act_function=<function ident at 0x061726A0>\n",
      " activations=[[0. 0. 0. 0. 0.]]\n",
      " biases=[[0. 0. 0. 0. 0.]]\n",
      " weights:\n",
      "     empty\n",
      "--------------------------\n",
      "[1]\n",
      " size=7\n",
      " act_function=<function sigmoid at 0x0A520778>\n",
      " activations=[[0. 0. 0. 0. 0. 0. 0.]]\n",
      " biases=[[0. 0. 0. 0. 0. 0. 0.]]\n",
      " weights:\n",
      "     |1 2 3 4 5 6 7 |\n",
      "     |1 2 3 4 5 6 7 |\n",
      "     |1 2 3 4 5 6 7 |\n",
      "     |1 2 3 4 5 6 7 |\n",
      "     |1 2 3 4 5 6 7 |\n",
      "\n",
      "--------------------------\n",
      "[2]\n",
      " size=7\n",
      " act_function=<function th at 0x0A5207C0>\n",
      " activations=[[0. 0. 0. 0. 0. 0. 0.]]\n",
      " biases=[[0. 0. 0. 0. 0. 0. 0.]]\n",
      " weights:\n",
      "     |1 2 3 4 5 6 7 |\n",
      "     |1 2 3 4 5 6 7 |\n",
      "     |1 2 3 -4 5 6 7 |\n",
      "     |1 2 3 4 5 6 7 |\n",
      "     |1 2 3 4 5 6 7 |\n",
      "     |1 2 3 4 5 6 7 |\n",
      "     |1 2 3 4 5 6 7 |\n",
      "\n",
      "--------------------------\n",
      "[3]\n",
      " size=5\n",
      " act_function=<function ident at 0x061726A0>\n",
      " activations=[[0. 0. 0. 0. 0.]]\n",
      " biases=[[1 1 1 1 1]]\n",
      " weights:\n",
      "     |1 1 0 1 1 |\n",
      "     |1 1 1 1 1 |\n",
      "     |1 1 1 0 1 |\n",
      "     |1 1 1 1 1 |\n",
      "     |1 1 1 1 1 |\n",
      "     |1 1 1 1 1 |\n",
      "     |1 1 1 1 1 |\n",
      "\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "net.insert_layer(0, 5, ident)\n",
    "net.insert_layer(1, 7, sigmoid)\n",
    "net.insert_layer(2, 7, th)\n",
    "net.insert_layer(3, 5, ident)\n",
    "\n",
    "net.init_weights(1, np.array([[1,2,3,4,5,6,7],[1,2,3,4,5,6,7],[1,2,3,4,5,6,7],[1,2,3,4,5,6,7],[1,2,3,4,5,6,7]]))\n",
    "net.init_weights(2, np.array([[1,2,3,4,5,6,7],[1,2,3,4,5,6,7],[1,2,3,-4,5,6,7],[1,2,3,4,5,6,7],[1,2,3,4,5,6,7],[1,2,3,4,5,6,7],[1,2,3,4,5,6,7]]))\n",
    "net.init_weights(3, np.array([[1,1,0,1,1],[1,1,1,1,1],[1,1,1,0,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]]))\n",
    "net.init_biases(3, np.array([[1,1,1,1,1]]))\n",
    "\n",
    "net.print_net_config()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[7.99999833, 7.99999833, 7.        , 6.99999833, 7.99999833]])"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.calc_output(np.array([1,2,0,2,1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fill activation derivatives array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "def calc_activations_derivatives(net: Net, target_vector):\n",
    "    last_layer = net.layers[net.layers_count - 1]\n",
    "    activation_der_array = []\n",
    "    last_layer_derivatives_array = []\n",
    "\n",
    "    for i in range(last_layer.neuron_count):\n",
    "        der = predict_error_der(last_layer.activations[0], target_vector, i)\n",
    "        last_layer_derivatives_array.append(der)\n",
    "\n",
    "    activation_der_array.insert(0, last_layer_derivatives_array)\n",
    "\n",
    "    # cal derivatives on each layer, except last and first layers\n",
    "    cur_iteration = 0\n",
    "    for i in range(net.layers_count - 2, 0, -1):\n",
    "        cur_layer = net.layers[i]\n",
    "        layer_derivatives_array = []\n",
    "\n",
    "        for j in range(cur_layer.neuron_count):\n",
    "            next_layer_der_array = activation_der_array[0]\n",
    "            layer_derivatives_array.append(net.der_cost_act(i, j, next_layer_der_array))\n",
    "\n",
    "        # add derivatives array of the current layer\n",
    "        activation_der_array.insert(0, layer_derivatives_array)\n",
    "        cur_iteration += 1\n",
    "\n",
    "    return activation_der_array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Net training functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "def get_weight_gradient_matrix(net: Net, layer_idx, activation_der_array):\n",
    "    assert 0 < layer_idx < net.layers_count\n",
    "    prev_layer_neuron_count = net.layers[layer_idx - 1].neuron_count\n",
    "    cur_layer_neuron_count = net.layers[layer_idx].neuron_count\n",
    "\n",
    "    gradient_matrix = np.zeros((prev_layer_neuron_count, cur_layer_neuron_count))\n",
    "\n",
    "    for i in range(prev_layer_neuron_count):\n",
    "        for j in range(cur_layer_neuron_count):\n",
    "            gradient_matrix[i][j] = net.der_cost_weigh(layer_idx, i, j, activation_der_array[layer_idx - 1][j])\n",
    "    return gradient_matrix\n",
    "\n",
    "def get_bias_gradient_matrix(net: Net, layer_idx, activation_der_array):\n",
    "    assert 0 < layer_idx < net.layers_count\n",
    "    cur_layer_neuron_count = net.layers[layer_idx].neuron_count\n",
    "    gradient_vector = np.zeros(cur_layer_neuron_count)\n",
    "\n",
    "    for j in range(cur_layer_neuron_count):\n",
    "        gradient_vector[j] = net.der_cost_bias(layer_idx, j, activation_der_array[layer_idx - 1][j])\n",
    "\n",
    "    return gradient_vector\n",
    "\n",
    "def step(net: Net, weight_grad_matrices, biases_grad_matrices):\n",
    "    for i in range(net.layers_count - 1):\n",
    "        idx = i + 1\n",
    "        cur_layer = net.layers[idx]\n",
    "        cur_layer.weights -= weight_grad_matrices[idx - 1]\n",
    "        cur_layer.biases -= biases_grad_matrices[idx - 1]\n",
    "\n",
    "def training_iteration(training_data_item, target_vector, net: Net):\n",
    "    net.calc_output(training_data_item)\n",
    "    act_der_array = calc_activations_derivatives(net, target_vector)\n",
    "\n",
    "    weight_grad_matrices = []\n",
    "    biases_grad_matrices = []\n",
    "\n",
    "    for i in range(net.layers_count - 1):\n",
    "        idx = i + 1\n",
    "        weight_grad_matrices.append(get_weight_gradient_matrix(net, idx, act_der_array))\n",
    "        biases_grad_matrices.append(get_bias_gradient_matrix(net, idx, act_der_array))\n",
    "    # change net parameters\n",
    "    step(net, weight_grad_matrices, biases_grad_matrices)\n",
    "\n",
    "def calc_metric(net: Net, val_input_vectors, val_target_vectors):\n",
    "    predict_errors = []\n",
    "\n",
    "    idx = 0\n",
    "    for target_vector in val_target_vectors:\n",
    "        input_vector = val_input_vectors[idx]\n",
    "        output = net.calc_output(input_vector)\n",
    "        predict_errors = np.append(predict_errors, predict_error(output, target_vector))\n",
    "\n",
    "    return np.mean(predict_errors)\n",
    "\n",
    "# net training on all training data items (returns metric on validation)\n",
    "def training(net: Net, training_data, target_vectors, val_input_vectors, val_target_vectors):\n",
    "    for training_item, target_vector in zip(training_data, target_vectors):\n",
    "        training_iteration(training_item, target_vector, net)\n",
    "    return calc_metric(net, val_input_vectors, val_target_vectors)\n",
    "\n",
    "# init weights and biases with start values\n",
    "def init_net_parameters(net: Net):\n",
    "    for i in range(net.layers_count - 1):\n",
    "        idx = i + 1\n",
    "        w_shape = net.layers[idx].weights.shape\n",
    "        b_shape = net.layers[idx].biases.shape\n",
    "\n",
    "        interval = (-0.5, 0.5)\n",
    "        delta = interval[1] - interval[0]\n",
    "\n",
    "        net.layers[idx].weights = rand.rand(w_shape[0], w_shape[1]) * delta + interval[0]\n",
    "        net.layers[idx].biases = rand.rand(b_shape[0], b_shape[1]) * delta + interval[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## generation of training data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "periods_count = 100\n",
    "epsilon = 0.1\n",
    "st_points_count = 10\n",
    "\n",
    "dst_dir = \"../data_sets/\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "training_data_file = open(dst_dir + \"cos_values_data\", \"w\")\n",
    "\n",
    "# 1 - write extremum points\n",
    "training_data_file.write(str(0) + \" \" + str(1) + \"\\n\")\n",
    "\n",
    "for i in range(periods_count - 1):\n",
    "    x = (np.pi / 2) * (i + 1)\n",
    "\n",
    "    val1 = np.cos(x)\n",
    "    val2 = np.cos(-x)\n",
    "    training_data_file.write(str(x) + \" \" + str(val1) + \"\\n\" + str(-x) + \" \" + str(val2) + \"\\n\")\n",
    "\n",
    "# write other points\n",
    "st_points = np.linspace(epsilon, np.pi / 2 - epsilon, st_points_count)\n",
    "\n",
    "for st_point in st_points:\n",
    "    for i in range(periods_count):\n",
    "        x = st_point + (np.pi / 2) * i\n",
    "        val1 = np.cos(x)\n",
    "        val2 = np.cos(-x)\n",
    "        training_data_file.write(str(x) + \" \" + str(val1) + \"\\n\" + str(-x) + \" \" + str(val2) + \"\\n\")\n",
    "\n",
    "training_data_file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load training data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before sorting: ['0 1\\n', '1.5707963267948966 6.123233995736766e-17\\n', '-1.5707963267948966 6.123233995736766e-17\\n', '3.141592653589793 -1.0\\n', '-3.141592653589793 -1.0\\n']\n",
      "after sorting: ['-156.97963267948967 0.9950041652780265\\n', '-156.82732197651245 0.9683381584748817\\n', '-156.67501127353523 0.9192515044496723\\n', '-156.52270057055804 0.8488807426887266\\n', '-156.37038986758083 0.7588552193149422\\n']\n",
      "-------------------------\n",
      "first 5 training items: [[-156.97963268]\n",
      " [-156.82732198]\n",
      " [-156.67501127]\n",
      " [-156.52270057]\n",
      " [-156.37038987]]\n",
      "first 5 training target vectors: [[0.99500417]\n",
      " [0.96833816]\n",
      " [0.9192515 ]\n",
      " [0.84888074]\n",
      " [0.75885522]]\n"
     ]
    }
   ],
   "source": [
    "training_data_file = open(dst_dir + \"cos_values_data\", \"r\")\n",
    "training_data = np.array([])\n",
    "target_vectors = np.array([])\n",
    "lines = []\n",
    "items_count = 0\n",
    "\n",
    "# read lines and shuffle it\n",
    "for line in training_data_file:\n",
    "    lines.append(line)\n",
    "\n",
    "print(\"before sorting: \" + str(lines[0:5]))\n",
    "lines = sorted(lines, key=lambda l: float(l.split()[0]))\n",
    "print(\"after sorting: \" + str(lines[0:5]))\n",
    "print(\"-------------------------\")\n",
    "\n",
    "# get training data items and target vectors:\n",
    "for line in lines:\n",
    "    arg, value = line.split()\n",
    "    arg = float(arg)\n",
    "    value = float(value)\n",
    "\n",
    "    training_data = np.append(training_data, arg)\n",
    "    target_vectors = np.append(target_vectors, value)\n",
    "    items_count += 1\n",
    "\n",
    "# reshape data to array of 1-d vectors\n",
    "training_data = training_data.reshape((items_count, 1))\n",
    "target_vectors = target_vectors.reshape((items_count, 1))\n",
    "\n",
    "print(\"first 5 training items: \" + str(training_data[0:5]))\n",
    "print(\"first 5 training target vectors: \" + str(target_vectors[0:5]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split training data on batches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches' count: 22\n",
      "---------\n",
      "training data batch 1:\n",
      "[[-142.69015504]\n",
      " [-142.53784433]\n",
      " [-142.38553363]\n",
      " [-142.23322293]\n",
      " [-142.08091222]\n",
      " [-141.92860152]\n",
      " [-141.77629082]\n",
      " [-141.62398011]\n",
      " [-141.47166941]\n",
      " [-141.37166941]\n",
      " [-141.27166941]\n",
      " [-141.11935871]\n",
      " [-140.96704801]\n",
      " [-140.8147373 ]\n",
      " [-140.6624266 ]\n",
      " [-140.5101159 ]\n",
      " [-140.35780519]\n",
      " [-140.20549449]\n",
      " [-140.05318379]\n",
      " [-139.90087308]\n",
      " [-139.80087308]\n",
      " [-139.70087308]\n",
      " [-139.54856238]\n",
      " [-139.39625168]\n",
      " [-139.24394098]\n",
      " [-139.09163027]\n",
      " [-138.93931957]\n",
      " [-138.78700887]\n",
      " [-138.63469816]\n",
      " [-138.48238746]\n",
      " [-138.33007676]\n",
      " [-138.23007676]\n",
      " [-138.13007676]\n",
      " [-137.97776605]\n",
      " [-137.82545535]\n",
      " [-137.67314465]\n",
      " [-137.52083395]\n",
      " [-137.36852324]\n",
      " [-137.21621254]\n",
      " [-137.06390184]\n",
      " [-136.91159113]\n",
      " [-136.75928043]\n",
      " [-136.65928043]\n",
      " [-136.55928043]\n",
      " [-136.40696973]\n",
      " [-136.25465903]\n",
      " [-136.10234832]\n",
      " [-135.95003762]\n",
      " [-135.79772692]\n",
      " [-135.64541621]\n",
      " [-135.49310551]\n",
      " [-135.34079481]\n",
      " [-135.1884841 ]\n",
      " [-135.0884841 ]\n",
      " [-134.9884841 ]\n",
      " [-134.8361734 ]\n",
      " [-134.6838627 ]\n",
      " [-134.531552  ]\n",
      " [-134.37924129]\n",
      " [-134.22693059]\n",
      " [-134.07461989]\n",
      " [-133.92230918]\n",
      " [-133.76999848]\n",
      " [-133.61768778]\n",
      " [-133.51768778]\n",
      " [-133.41768778]\n",
      " [-133.26537707]\n",
      " [-133.11306637]\n",
      " [-132.96075567]\n",
      " [-132.80844497]\n",
      " [-132.65613426]\n",
      " [-132.50382356]\n",
      " [-132.35151286]\n",
      " [-132.19920215]\n",
      " [-132.04689145]\n",
      " [-131.94689145]\n",
      " [-131.84689145]\n",
      " [-131.69458075]\n",
      " [-131.54227004]\n",
      " [-131.38995934]\n",
      " [-131.23764864]\n",
      " [-131.08533794]\n",
      " [-130.93302723]\n",
      " [-130.78071653]\n",
      " [-130.62840583]\n",
      " [-130.47609512]\n",
      " [-130.37609512]\n",
      " [-130.27609512]\n",
      " [-130.12378442]\n",
      " [-129.97147372]\n",
      " [-129.81916302]\n",
      " [-129.66685231]\n",
      " [-129.51454161]\n",
      " [-129.36223091]\n",
      " [-129.2099202 ]\n",
      " [-129.0576095 ]\n",
      " [-128.9052988 ]\n",
      " [-128.8052988 ]\n",
      " [-128.7052988 ]\n",
      " [-128.55298809]]\n",
      "size=100\n",
      "---------\n",
      "target vectors batch 1:\n",
      "[[-2.49642166e-01]\n",
      " [-3.93670765e-01]\n",
      " [-5.28584416e-01]\n",
      " [-6.51259362e-01]\n",
      " [-7.58855219e-01]\n",
      " [-8.48880743e-01]\n",
      " [-9.19251504e-01]\n",
      " [-9.68338158e-01]\n",
      " [-9.95004165e-01]\n",
      " [-1.00000000e+00]\n",
      " [-9.95004165e-01]\n",
      " [-9.68338158e-01]\n",
      " [-9.19251504e-01]\n",
      " [-8.48880743e-01]\n",
      " [-7.58855219e-01]\n",
      " [-6.51259362e-01]\n",
      " [-5.28584416e-01]\n",
      " [-3.93670765e-01]\n",
      " [-2.49642166e-01]\n",
      " [-9.98334166e-02]\n",
      " [-3.43210594e-15]\n",
      " [ 9.98334166e-02]\n",
      " [ 2.49642166e-01]\n",
      " [ 3.93670765e-01]\n",
      " [ 5.28584416e-01]\n",
      " [ 6.51259362e-01]\n",
      " [ 7.58855219e-01]\n",
      " [ 8.48880743e-01]\n",
      " [ 9.19251504e-01]\n",
      " [ 9.68338158e-01]\n",
      " [ 9.95004165e-01]\n",
      " [ 1.00000000e+00]\n",
      " [ 9.95004165e-01]\n",
      " [ 9.68338158e-01]\n",
      " [ 9.19251504e-01]\n",
      " [ 8.48880743e-01]\n",
      " [ 7.58855219e-01]\n",
      " [ 6.51259362e-01]\n",
      " [ 5.28584416e-01]\n",
      " [ 3.93670765e-01]\n",
      " [ 2.49642166e-01]\n",
      " [ 9.98334166e-02]\n",
      " [-1.42089978e-14]\n",
      " [-9.98334166e-02]\n",
      " [-2.49642166e-01]\n",
      " [-3.93670765e-01]\n",
      " [-5.28584416e-01]\n",
      " [-6.51259362e-01]\n",
      " [-7.58855219e-01]\n",
      " [-8.48880743e-01]\n",
      " [-9.19251504e-01]\n",
      " [-9.68338158e-01]\n",
      " [-9.95004165e-01]\n",
      " [-1.00000000e+00]\n",
      " [-9.95004165e-01]\n",
      " [-9.68338158e-01]\n",
      " [-9.19251504e-01]\n",
      " [-8.48880743e-01]\n",
      " [-7.58855219e-01]\n",
      " [-6.51259362e-01]\n",
      " [-5.28584416e-01]\n",
      " [-3.93670765e-01]\n",
      " [-2.49642166e-01]\n",
      " [-9.98334166e-02]\n",
      " [ 3.42839206e-15]\n",
      " [ 9.98334166e-02]\n",
      " [ 2.49642166e-01]\n",
      " [ 3.93670765e-01]\n",
      " [ 5.28584416e-01]\n",
      " [ 6.51259362e-01]\n",
      " [ 7.58855219e-01]\n",
      " [ 8.48880743e-01]\n",
      " [ 9.19251504e-01]\n",
      " [ 9.68338158e-01]\n",
      " [ 9.95004165e-01]\n",
      " [ 1.00000000e+00]\n",
      " [ 9.95004165e-01]\n",
      " [ 9.68338158e-01]\n",
      " [ 9.19251504e-01]\n",
      " [ 8.48880743e-01]\n",
      " [ 7.58855219e-01]\n",
      " [ 6.51259362e-01]\n",
      " [ 5.28584416e-01]\n",
      " [ 3.93670765e-01]\n",
      " [ 2.49642166e-01]\n",
      " [ 9.98334166e-02]\n",
      " [ 7.35221366e-15]\n",
      " [-9.98334166e-02]\n",
      " [-2.49642166e-01]\n",
      " [-3.93670765e-01]\n",
      " [-5.28584416e-01]\n",
      " [-6.51259362e-01]\n",
      " [-7.58855219e-01]\n",
      " [-8.48880743e-01]\n",
      " [-9.19251504e-01]\n",
      " [-9.68338158e-01]\n",
      " [-9.95004165e-01]\n",
      " [-1.00000000e+00]\n",
      " [-9.95004165e-01]\n",
      " [-9.68338158e-01]]\n",
      "size=100\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "data_size = len(training_data)\n",
    "batch_count = data_size // batch_size\n",
    "rem = data_size % batch_size\n",
    "training_data_batches = []\n",
    "target_vectors_batches = []\n",
    "\n",
    "if data_size % batch_size != 0:\n",
    "    batch_count += 1\n",
    "\n",
    "for i in range(batch_count):\n",
    "    pos = i * batch_size\n",
    "    tr_data_batch = []\n",
    "    t_vectors_batch = []\n",
    "\n",
    "    if i == batch_count - 1:\n",
    "        tr_data_batch = training_data[pos:]\n",
    "        t_vectors_batch = target_vectors[pos:]\n",
    "    else:\n",
    "        tr_data_batch = training_data[pos:pos + batch_size]\n",
    "        t_vectors_batch = target_vectors[pos:pos + batch_size]\n",
    "    training_data_batches.append(tr_data_batch)\n",
    "    target_vectors_batches.append(t_vectors_batch)\n",
    "\n",
    "print(\"batches' count: \" + str(batch_count))\n",
    "print(\"---------\")\n",
    "idx = 1\n",
    "print(\"training data batch \" + str(idx) + \":\\n\" + str(training_data_batches[idx]))\n",
    "print(\"size=\" + str(len(training_data_batches[idx])))\n",
    "\n",
    "print(\"---------\")\n",
    "print(\"target vectors batch \" + str(idx) + \":\\n\" + str(target_vectors_batches[idx]))\n",
    "print(\"size=\" + str(len(target_vectors_batches[idx])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create net and train it"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# net parameters\n",
    "net = Net()\n",
    "hidden_layers_size = 10\n",
    "hidden_layers_count = 3\n",
    "activation = th\n",
    "\n",
    "net.insert_layer(0, 1, ident)\n",
    "\n",
    "for i in range(hidden_layers_count):\n",
    "    net.insert_layer(i + 1, hidden_layers_size, activation)\n",
    "net.insert_layer(hidden_layers_count + 1, 1, activation)\n",
    "#print(net.layers_count)\n",
    "\n",
    "for i in range(hidden_layers_count):\n",
    "    idx = i + 1\n",
    "    cur_layer = net.layers[idx]\n",
    "\n",
    "    if i == 0:\n",
    "        cur_layer.weights = np.zeros((1, hidden_layers_size))\n",
    "    else:\n",
    "        cur_layer.weights = np.zeros((hidden_layers_size, hidden_layers_size))\n",
    "    cur_layer.biases = np.zeros((1, hidden_layers_size))\n",
    "\n",
    "net.init_weights(net.layers_count - 1, np.zeros((hidden_layers_size, 1)))\n",
    "net.init_biases(net.layers_count - 1, np.zeros((1, 1)))\n",
    "\n",
    "# init all net parameters with values from standart normal destribution\n",
    "init_net_parameters(net)\n",
    "\n",
    "#net.print_net_config()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: 0.6397932838694731\n",
      "epoch 1: 0.9394526132068173\n",
      "epoch 2: 0.698411583008098\n",
      "epoch 3: 0.9355425904951062\n",
      "epoch 4: 0.7859180582560614\n",
      "epoch 5: 0.8871797430669052\n",
      "epoch 6: 0.8688935944467461\n",
      "epoch 7: 0.7003706107269546\n",
      "epoch 8: 0.901158833364851\n",
      "epoch 9: 0.6319415395937856\n",
      "epoch 10: 0.8728017333860119\n",
      "epoch 11: 0.6333726185163615\n",
      "epoch 12: 0.7406113482223368\n",
      "epoch 13: 0.668217485978285\n",
      "epoch 14: 0.7032543867438426\n",
      "epoch 15: 0.696823803219905\n",
      "epoch 16: 0.6939520978505145\n",
      "epoch 17: 0.6810975354276543\n",
      "epoch 18: 0.6849934035058579\n",
      "epoch 19: 0.7335767430840453\n",
      "epoch 20: 0.636298883441533\n",
      "epoch 21: 0.6506095069256727\n"
     ]
    }
   ],
   "source": [
    "min_cost = 0.05\n",
    "saved_nets = []\n",
    "\n",
    "for i in range(batch_count):\n",
    "    training_batch = training_data_batches[i]\n",
    "    target_vectors_batch = target_vectors_batches[i]\n",
    "\n",
    "    cost = training(net, training_batch, target_vectors_batch, training_data, target_vectors)\n",
    "    saved_nets.append((copy.deepcopy(net), cost))\n",
    "\n",
    "    print(\"epoch \" + str(i) + \": \" + str(cost))\n",
    "\n",
    "    if cost <= min_cost:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get best net config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric of the best net - 0.6319415395937856\n"
     ]
    }
   ],
   "source": [
    "min = 10000\n",
    "result = 0\n",
    "\n",
    "for item in saved_nets:\n",
    "    if item[1] < min:\n",
    "        result = item\n",
    "        min = item[1]\n",
    "\n",
    "result_net = result[0]\n",
    "\n",
    "# check net cost function\n",
    "print(\"metric of the best net - \" + str(calc_metric(result_net, training_data, target_vectors)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}